% \begin{itemize}
%     \item explain RfA data collection 
%     \begin{itemize}
%         \item existing SNAP data and limitations
%         \item XML parsing 
%         \item regex and string matching 
%         \item date parsing
%     \end{itemize}
%     \item Social interactions 
%     \begin{itemize}
%         \item User contributions
%         \item wealth and diversity of info 
%         \item creating underlying network
%     \end{itemize}   
% \end{itemize}

\section{Dataset}
\label{sec:dataset}

We would like to have two different types of data to help build the election model in this paper. The first would be information of the votes cast in a RfA and the eventual result of the RfA. This gives us the users interactions in an online election process where they have to judge their peers. The second, is information on the interactions of users in other non-elections settings. In Wikipedia discussions occur in \textit{Talk Pages}. Every type of Wikipedia page (articles, user pages, help pages etc) has a \textit{Talk Page} where users can discuss the contents of that article or interact with the user or provide information to others. These are valuable data sources to gather more details on user activities.

In this section we will discuss the existing Wikipeida datasets from Stanford large network datasets (SNAP) \cite{snapnets} that satisfy our requirements and their inherent limitations. Next we will illustrate the process by which we collected newer data from Wikipedia.


\subsection{Existing Datasets}
For the first type of data we requore thre are two existing Wikipedia RfA datasets in SNAP namely \wikielect a
nd \wikirfa. They both contain attributes of each vote in a RfA such as the source, target, vote , result of RfA, timestamp. The \wikirfa is a more recent version of the \wikielect dataset. It has RfAs till May 2013 and also has the comment text of voters. There are 11,000 users and around 190 thousand votes in total. Both of these datasets have been used in many previous works mostly as signed networks. There are a few limitations of these datasets when we would like to analyze them as vote cast in an election. There is more than 5\% of \wikirfa votes that have no timestamp and almost 1\% of votes that have no source. As most RfAs have fewer than 300 votes this is an issue when considering the sequence of votes as well as who has cast a vote.

The interactions between users outside of RfA elections is useful to understand behaviour and perceptions of others. Wikipedia users can directly interact with another user by writing on their \textit{User Talk Page}. This can be a measure of how much correspondence exists between two users. We saw how this is a good indication of probability of supporting a candidate for an election \cite{leskovec2010governance}. The \textit{wiki-Talk} dataset on SNAP contains a directed network where an edge from node \textit{u} to \textit{v} signifies that \textit{u} has written in \textit{v}'s talk page. This dataset is a large network containing more than 2 million nodes and 5 million edges. The limitation of this network data is that nodes do not have user id mapping and also the edges are not weighted. Without a node to user id mapping the network canot be used with the election data. Having weighted edges tells us how many times a user has interacted with someone else which is more informative. 
\smallskip

Due to the limitations of the existing datasets we set out to collect our own data to build our election model.

\subsection{RfA Data Collection}

We went through a 60GB XML dump of Wikipedia from Jan 2019 to extract the RfA data. We chose to scrape the data in a format similar to the SNAP \wikirfa dataset. The outline of the data extraction process is illustrated in Figure~\ref{fig:rfa-extraction}. In the first step we filter out all Wiki pages whose title doesn't contain the term \texttt{Requests for adminship}. This still leaves us with a lot of non-election Wiki pages, so we can further filter by checking for terms such as \texttt{Category:Unsuccessful requests} or \texttt{Category:Successful requests}.Now this reduces the space from over 5 million pages to the roughly 4000 pages related to RfA elections.

The next step is to process the body of the election pages individually and extract votes from the \wikitalk, Wikipedia's own markup syntax. After locating the \texttt{Support}, \texttt{Oppose} and \texttt{Neutral} sections we can extract the individual votes. This step is particularly hard as \wikitalk syntax changes through the years and there is no fixed page structure. The user's comment can also nested discussion threads which we chose to not extract. As user vote are ended with a signature, i.e their user id and timestamp. The timestamps also have varied syntaxes adding to the overall complexity of this extraction phase. Using more robust regular expressions to capture multiple timestamp formats and also handling a myraid of edgge cases in processing we acheive a much higher coverage of election votes. 

We collected $226,781$ votes from $4,557$ elections with over $13,000$ unique user ids. Only $1.6\%$ of votes have missing timestamps and $0.4\%$ have a missing source. We also added unique id (UID) field to differentiate candidates who stood for elections multiple times. 

\tikzset
{mybox/.style=
  {rectangle,rounded corners,drop shadow,minimum height=1cm,
   minimum width=2cm,align=center,fill=#1,draw=colBorder,line width=1pt
  },
 myarrow/.style=
  {draw=#1,line width=3pt,-stealth,rounded corners
  },
 mylabel/.style={text=#1}
}
\begin{figure*}[h!]
    \centering
    \begin{tikzpicture}
        [align=center,node distance=2cm]
        \node[mybox=colD] (W) {Complete Wiki\\ XML Dump};
        \node[mybox=colIP,right=of W] (F) {RfA related\\ XML pages};
        \node[mybox=colV,right=of F] (V) {Final Dataset};
        \path[myarrow=colD] (W) -- node[above] {Filter} (F);
        \path[myarrow=colIP] (F) -- node[above] {Extract} node[below] {Votes} (V);
    \end{tikzpicture}
    
    \caption{RfA Data Collection Process}
    \label{fig:rfa-extraction}
\end{figure*}


\subsection{User Interaction Data Collection}
\label{sec:user-contrib}
Wikipedia has an API to request all the contributions made by a particular user \cite{wiki:Usercontribs}. This offers a rich source of data on the activities of a user on Wikipedia as seen in Figure~\ref{fig:edit-summary} from the online \textit{editsummary tool}\footnote{\url{https://xtools.wmflabs.org/editsummary}} for Wikipedia users. We proceeded to collect the contributions for every unique user in the RfA data querying the API. There are some issues with the user ids that are present in the RfA data. As a single user can have multiple aliases and/or change their user id at any point, some users might not have any contributions under an alias that has been discontinued. To simplify our data colection, we assume that each user id is a unique user and will fetch contributions under that user id if present. This resulted in 100GB of data for nearly $11,000$ out of $13,000$ user ids. We will refer to this dataset \usercontrib from this point.

We can see that edits in \usercontrib have a \textit{namespace} as seen in \ref{fig:namespace-totals}. These are categories for each Wiki page like \texttt{Main} is for all the articles on Wikipedia and \texttt{User Talk} is available for each user. Each category also has a the corresponding \textit{Talk Page} for discussions. Therefore, we can get user interactions by looking at the \texttt{User Talk} namespace. As an example in \ref{fig:user-talk-edits}, the top edited user talk page is of \texttt{Dianna} (The actual user is \texttt{Justlettersandnumbers}, hence the top results are edits on their own page). This allows us to create a dataset similar to  the \wikitalk  dataset, with user id mappings as well as count of number of interactions. The data on top edited pages as seen in Figure~\ref{fig:top-edits} can also be used to create a profile of a user's diversity or speciality of topics and much more.

